# .github/workflows/daily_scrape.yml
#
# Runs the Statcast scraper every day at 8am ET (1pm UTC) during baseball season.
# Also runs on push to main (for testing) and can be triggered manually.
#
# Required GitHub Secrets:
#   SUPABASE_URL          — from Supabase project settings → API
#   SUPABASE_SERVICE_KEY  — service_role key (NOT anon key) from same page

name: Daily Statcast Scrape

on:
  schedule:
    # 1:00 PM UTC = 8:00 AM ET (9:00 AM EDT during daylight saving)
    - cron: '0 13 * * *'
  workflow_dispatch:   # allow manual trigger from GitHub UI
  push:
    branches: [main]
    paths:
      - 'scraper/**'   # only re-run if scraper code changes

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Cache pip packages
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('scraper/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          pip install -r scraper/requirements.txt

      - name: Run scraper
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
        run: |
          python scraper/scrape.py

      - name: Notify on failure
        if: failure()
        run: |
          echo "Scraper failed on $(date). Check GitHub Actions logs."
          # Optional: add Slack/email notification here
